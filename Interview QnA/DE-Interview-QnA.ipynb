{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Data Engineering Questions -- Spark"
            ],
            "metadata": {
                "azdata_cell_guid": "2b816d78-bd9a-4ad9-af77-555e740afa6a"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Q1. Why Apache Spark is an efficient data processing framework as compared to Hadoop?\n",
                "\n",
                "A1. Spark is a distributed computing platform and is built for efficiency and speed. <span style=\"color: var(--vscode-foreground);\">To better understand this we have to understand problems that Spark tries to solve.</span>\n",
                "\n",
                "1. **_Iterative Processing_** -- Hadoop uses HDFS to process intermediate results (write to Disk n Read from Disk) which is ingerently slow. On the other hand Spark uses flash memory to hold these intermediate results which improves efficiency.\n",
                "2. **_Interactive Processing_** -- Spark has efficient implementation of Map n Reduce program and provides a rich library to perform various operations on Data abstracting Map Reduce complexity from user (unlike Hadoop). Spark will keep data in cache for which we want to analyze and operate on, keeping data in cache increases performamnce.\n",
                "\n",
                "To summarize Spark provides distributed In-Memory computing capabilities. Spark execution engine is another feature that makes spark faster.  \n",
                "\n",
                "Spark execution engine take the user commands and convert it into Optimized logical plan \\>\\> Physical Plan \\>\\> Tasks  \n",
                "\n",
                "one can say spark does not simply executes user inputs like Hadoop(without analysing for performance) but keep track of each user input and executes it in optimized way, keeping track of all the operations and transformations of data."
            ],
            "metadata": {
                "azdata_cell_guid": "44c9a4e3-790f-4d33-88f6-9b93aeb379dd"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Q2. Explain the main components of Apache Spark Architecture and how they interact with each other ?\n",
                "\n",
                "A2. Spark uses a group of computers to perform computation in a Master - Slave configuration. Main components of Spark framework / architecture :--\n",
                "\n",
                "1. _Driver(Master)_ -- responsible for spark application execution across worker nodes. This is the entry point for spark app. and provides context for execution.\n",
                "2. _Cluster Manager_ -- Manages resources on the cluster and allocate them to different spark apps.\n",
                "3. _Worker (Slaves)_ -- Executes the code as submitted by spark apps.\n",
                "4. _Executors_ -- are the worker process that reside on Worker nodes and carry out commands / tasks.\n",
                "5. _Tasks_ -- smallest unit of work that can work on single partition of data.\n",
                "\n",
                "Spark architecture abstractions :--\n",
                "\n",
                "1. [_Resilient Distributed Dataset (RDD)_](.\\RDD.ipynb)\n",
                "2. [_Distributed Acyclic Graph (DAG)_](.\\DAG.ipynb)"
            ],
            "metadata": {
                "azdata_cell_guid": "2501f2d1-0d5f-4aa9-af03-858b580db4be"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Q3. What exactly will happen if you suddenly stop the spark context?\n",
                "\n",
                "A3. The Spak Jobs will fail executing under this Spark Context.\n",
                "\n",
                "Spark context status could not be checked before version 1.6. For version 1.6 and beyond use below command to get status of Spark Context\n",
                "\n",
                "- <span style=\"font-size: 13px; white-space-collapse: preserve;\">PySpark -- sc._jsc.sc().isStopped()</span>\n",
                "- <span style=\"font-size: 13px; white-space-collapse: preserve;\">Scala -- sc.isStopped; spark.sparkContext.isStopped</span>"
            ],
            "metadata": {
                "azdata_cell_guid": "880cf2a0-4ee8-4b46-9888-fd2e74f6524c"
            },
            "attachments": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "###"
            ],
            "metadata": {
                "azdata_cell_guid": "0b6a18b5-23e9-4e7d-ad8e-5f43b05cc291"
            },
            "attachments": {}
        }
    ]
}